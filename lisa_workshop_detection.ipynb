{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gravitational wave detection with neural networks\n",
    "\n",
    "[Joe Bayley](mailto:joseph.bayley@glasgow.ac.uk)\n",
    "[Michael J Williams](mailto:m.williams.4@research.gla.ac.uk)\n",
    "\n",
    "Institute for Gravitational Research \n",
    "\n",
    "University of Glasgow\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"https://raw.githubusercontent.com/mj-will/intro2ml/master/docs/assets/igr.jpg\" width=\"100\"/></div>\n",
    "\n",
    "Machine learning, in particular deep learning methods have been demonstrated to be able to identify gravitational wave signals quite effectively. A few notable examples for detections with LIGO include:\n",
    "[Gabbard et al](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.120.141103) where convolutional neural networks are used to detect simulated signals from binary black hole coalescence in noisy data. \n",
    "\n",
    "\n",
    "In this tutorial we will use a basic convolutional neural network to identify some massive black hole binary signals within LISA data.\n",
    "\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/jcbayley/lisamlworkshop/blob/main/lisa_workshop_detection.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set is a simplified parameter space for MBHB as the more complex the parameter space, the more data is needed and a more complex network is likely needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we will be using Massive Black Hole Binary signals, and have the condition that they do not overlap and that they exist purely within coloured Gaussian noise based on a projected LISA PSD. This simplifies the problem greatly for the case of detection. Also to reduce the complexity of the problem, many of the parameters for the MBHB will be fixed and only some ramdomly sampled, this again reduces the size of the parameter space and the complexity of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior space\n",
    "\n",
    "The prior space for the MBHB is defined in the table below:\n",
    "\n",
    "| name| symbol| range| info|\n",
    "|-----|-----|-----|-----|\n",
    "|component mass | $m_1, m_2$| $[5\\times10^7, 10^8]$ $M_{\\odot}$| |\n",
    "|dimensionless spin | $\\chi_{1,2}$| $0$| |\n",
    "|Luminosity distance |$D_L$| $[0.5, 100]$ Gpc| |\n",
    "|merger time |$t_c$| $0.75T$| |\n",
    "|inclination | $\\theta_{jn}$ | $0$| angle between total angular momentum (J) and line of sight(n)|\n",
    "|phase at merger | $\\phi_c$ | $0$| |\n",
    "|polarisation | $\\psi$ | $0$| |\n",
    "|longitude | $\\phi$ | $0$| |\n",
    "|cos latitude |$\\cos \\theta$| $0$| |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the size of the dataset further and simplify the problem for this notebook, we have also used a low sampling rate and duration\n",
    "\n",
    "| name| value|\n",
    "| -----| ---- |\n",
    "| sampling frequency| $1$ mHz|\n",
    "| duration | $204800$ s (2.3 days)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also choose to use the TDI outputs X,Y,Z as input to the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing and importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages if not already installed\n",
    "try:\n",
    "    import bilby\n",
    "except ImportError:\n",
    "    !pip install bilby[gw]\n",
    "    !pip install pytdi\n",
    "    !pip install scikit-learn\n",
    "    !pip install scipy\n",
    "    !pip install lisagwresponse\n",
    "    # Kill the kernel so the list of packages updates\n",
    "    import os\n",
    "    os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to restart the kernel after installing the packages. The previous cell will kill the kernel, so you now need to rerun the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.signal\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import bilby\n",
    "import pytdi.michelson\n",
    "import lisagwresponse\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_context(\"notebook\")\n",
    "sns.set_palette(\"colorblind\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(url, filename):\n",
    "    \"\"\"Fetch data from a url and save it to given file\"\"\"\n",
    "    if not os.path.isfile(filename):\n",
    "        import urllib\n",
    "        urllib.request.urlretrieve(url, filename=filename)\n",
    "    else:\n",
    "        print(\"File already exists!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"data\", exist_ok=True)\n",
    "orbits_url = \"https://github.com/jcbayley/lisamlworkshop/raw/main/data/esa-orbits-1-0-2.h5\"\n",
    "orbits_file = \"data/esa-orbits-1-0-2.h5\"\n",
    "fetch_data(orbits_url, orbits_file)\n",
    "# Now for the training data\n",
    "data_url = \"https://github.com/jcbayley/lisamlworkshop/raw/main/data/sig_noise_data_0_2000.h5\"\n",
    "data_file = \"data/sig_noise_data.h5\"\n",
    "fetch_data(data_url, data_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matched Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will look at how to generate a waveform and we will use the bilby package, which is primarily a parameter estimation library, however has some useful data generation tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a function which uses the IMRPhenomD waveform generator, which generates binary black hole waveforms without precession, higher modes or individual component spins, to create the two polarisation (plus and cross) of a gravitational wave waveform.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_polarisations(injection_parameters, sampling_frequency, start_time, duration, minimum_frequency=1e-6):\n",
    "    \"\"\" Generates the polarisations of signal.\n",
    "    \"\"\"\n",
    "    # Fixed arguments passed into the source model \n",
    "    waveform_arguments = dict(waveform_approximant=\"IMRPhenomD\",\n",
    "                              reference_frequency=1e-3, \n",
    "                              minimum_frequency=minimum_frequency,\n",
    "                              maximum_frequency=sampling_frequency/2.0)\n",
    "\n",
    "    # Create the waveform_generator using a LAL BinaryBlackHole source function\n",
    "    waveform_generator = bilby.gw.WaveformGenerator(\n",
    "        duration=duration, sampling_frequency=sampling_frequency,\n",
    "        frequency_domain_source_model=bilby.gw.source.lal_binary_black_hole,\n",
    "        parameter_conversion=bilby.gw.conversion.convert_to_lal_binary_black_hole_parameters,\n",
    "        waveform_arguments=waveform_arguments,\n",
    "        start_time=start_time)\n",
    "    \n",
    "    # extract waveform from bilby\n",
    "    waveform_generator.parameters = injection_parameters\n",
    "    # get the time domain strain\n",
    "    time_signal = waveform_generator.time_domain_strain()\n",
    "\n",
    "    return time_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "injection_parameters = {\n",
    "    \"mass_1\": 1e6,              # solar masses\n",
    "    \"mass_2\": 1e6,              # solar masses\n",
    "    \"chi_1\": 0,                 # unitless effective spin \n",
    "    \"chi_2\": 0,\n",
    "    \"luminosity_distance\": 2e3, # Mpc\n",
    "    \"theta_jn\": 0,              # \n",
    "    \"phase\": 0,\n",
    "    \"polarisation\": 0,\n",
    "    \"lon\": 0,                   # radians\n",
    "    \"lat\":0                     # radians\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_frequency = 0.001 # low sampling frequency to keep number of samples low\n",
    "waveform_start_time = 22160.0 # start time is just after the orbits file starts\n",
    "waveform_duration = 10000000 # set the duration of the waveform \n",
    "data_duration = waveform_duration # can set the data duration to be a smaller section of waveform if necessary\n",
    "data_start_time = waveform_start_time + waveform_duration - data_duration + 0.25*data_duration # place the data start time of reponse such that the merger in 0.25 seconds before the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the array of times for the waveform and data \n",
    "waveform_times = np.arange(waveform_start_time, waveform_start_time + waveform_duration, 1./sampling_frequency)\n",
    "data_times = np.arange(data_start_time, data_start_time + data_duration, 1./sampling_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use bibly to create some IMRPhenomD waveforms\n",
    "pol = create_polarisations(injection_parameters, sampling_frequency, waveform_start_time, waveform_duration, minimum_frequency=8e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(waveform_times, pol[\"plus\"], label = \"Plus polarisation\")\n",
    "ax.plot(waveform_times, pol[\"cross\"], label = \"Cross polarisation\")\n",
    "ax.set_xlabel(\"Time [s]\")\n",
    "ax.set_ylabel(\"GW strain\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the lisa response at some times (data_times) given some waveform polarisations and sky position and orbits\n",
    "data = lisagwresponse.ReadStrain(waveform_times,                          # times at which the polarisations were generated\n",
    "                                pol[\"plus\"],                              # plus polarisations\n",
    "                                pol[\"cross\"],                             # cross polarisations\n",
    "                                gw_beta=injection_parameters[\"lat\"],      # latitude of source (ecliptic frame)\n",
    "                                gw_lambda=injection_parameters[\"lon\"],    # longitude in source\n",
    "                                orbits=orbits_file,                       # orbits file for lisa\n",
    "                                size = data_duration*sampling_frequency,  # number of samples in output responses\n",
    "                                dt = 1./sampling_frequency,               # separation of response samples\n",
    "                                t0 = data_start_time)                     # start time of the response outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the lisa response from one of the links at the specified times\n",
    "resp = data.compute_gw_response(data_times, data.LINKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(data_times, resp[:,0])\n",
    "ax.set_xlabel(\"Time [s]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to use pytdi with the outputs of lisagwresponse one needs to write the response to a file\n",
    "data.write(path=f\"./test_gw_0.h5\",mode=\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the tdi class with the response and orbits files\n",
    "tdi_data = pytdi.Data.from_gws(\"./test_gw_0.h5\", orbits=\"./data/esa-orbits-1-0-2.h5\", skipped=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the X tdi\n",
    "X2 = pytdi.michelson.X2.build(**tdi_data.args)\n",
    "X2_signal = X2(tdi_data.measurements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(data_times[100:-100], X2_signal, label=\"TDI - X\")\n",
    "ax.set_xlabel(\"Time [s]\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psd_mag = (2e-21)**2       # set a fixed psd magnitude\n",
    "Nt = len(X2_signal)        # define length of time series\n",
    "dt = 1./sampling_frequency # sample separation\n",
    "resp_duration = Nt*dt      # time duration \n",
    "df = 1./resp_duration      # frequency separation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some noise based on psd \n",
    "sigma = np.sqrt(Nt*psd_mag/(2*dt)) # define frequency domain standard deviation for discrete fft\n",
    "noise_fs = 0.5*sigma*(np.random.normal(0,1,size=Nt//2 + 1) + 1j*np.random.normal(0,1,size=Nt//2 + 1)) # generate complex Gaussian noise\n",
    "noise_ts = np.fft.irfft(noise_fs) # inverse fft to get time series\n",
    "data_X2 = X2_signal + noise_ts # add signal to noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(data_times[100:-100], data_X2, label=\"noisy data\")\n",
    "ax.plot(data_times[100:-100], X2_signal, alpha=0.5, label=\"signal only\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Time [s]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate  time frequency spectrogram of the noisy signal\n",
    "spect = scipy.signal.spectrogram(data_X2, fs=sampling_frequency, nperseg=128, noverlap=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(spect[2], origin=\"lower\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the matched filter we can use\n",
    "$$ (s | h) = 4 {\\rm Re} \\int_{0}^{\\infty} \\frac{\\tilde{s}(f) \\tilde{h}^*(f)}{S_{n(f)}}df $$\n",
    "The matched filter is then defined as \n",
    "$$ \\rho = \\frac{(s|h)}{\\sqrt{(h|h)}} $$\n",
    "Where $\\tilde{s}(f)$ is the Fourier transformed detector data, $\\tilde{h}^*(f)$ is the complex conjugate of the fourier transformed waveform and $S_{n}(f)$ is the noise power spectral density of the detector.\n",
    "We can find the SNR timeseries using\n",
    "$$ \\rho(t) = \\frac{4}{\\sqrt{(h|h)}} {\\rm Re} \\int_{0}^{\\infty} \\frac{\\tilde{s}(f) \\tilde{h}^*(f)}{S_{n}(f)} e^{2\\pi i f t}df$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement this as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_snr_timeseries(filter, data, psd, dt):\n",
    "    # define come constants\n",
    "    Nt = len(filter)\n",
    "    data_duration = Nt*dt\n",
    "\n",
    "    # compute data ffts\n",
    "    detector_fft = np.fft.rfft(data) * dt\n",
    "    waveform_fft = np.fft.rfft(filter) * dt\n",
    "    \n",
    "    #compute complex conjugate\n",
    "    finner =  np.conj(waveform_fft) * detector_fft / psd\n",
    "    #compute the optimal SNR\n",
    "    osnr_sum = 4 / data_duration * np.sum(np.conj(waveform_fft) * waveform_fft / psd)\n",
    "    # compute matched filter SNR for optimal waveform\n",
    "    snr_sum = 4 / data_duration * np.sum(finner) / np.sqrt(osnr_sum)\n",
    "\n",
    "    # compute the SNR timeseries\n",
    "    snr_time = 4 / data_duration * 0.5*Nt * np.fft.irfft(finner)/ np.sqrt(osnr_sum)\n",
    "\n",
    "    # roll the timeseries so template times match up\n",
    "    return np.roll(snr_time, -int(0.25*data_duration*sampling_frequency)), snr_sum, np.sqrt(osnr_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snr_time, matched_snr, opt_snr = compute_snr_timeseries(X2_signal, data_X2, psd_mag, dt)\n",
    "print(\"Matched filter SNR: \", np.real(matched_snr))\n",
    "print(\"Optimal SNR : \", np.real(opt_snr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(data_times[100:-100], abs(snr_time))\n",
    "#ax.plot(data_times[100:-100], abs(snr2.data), alpha = 0.5)\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.set_ylabel(\"SNR\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Data is available [here](http://www.astro.gla.ac.uk/users/michael/datasets/bbh/BBH_training_1s_1024Hz_10Ksamp_1n_iSNR8_Hdet_astromass_1seed_ts_0.pkl) but this notebook will automatically download it with the following function.\n",
    "\n",
    "The data consists of time series which will be labelled `x` and labels (no signal/signal) labelled `y`. The dimensions of the time series array will be:\n",
    "`[number of time series, tdi channels, number of samples]`\n",
    "We will change the dataset to match the default of torch which is to put the channels last to that:\n",
    "`[number of time series, number of samples, tdi channels]`\n",
    "\n",
    "The number of samples with depend on the duration and sampling rate, in this example the data is one \"1 month\" and sampled at 1Hz.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_file):\n",
    "    \"\"\"Load the data\"\"\"\n",
    "    # load the data\n",
    "    with h5py.File(data_file, 'r') as f:\n",
    "        x = np.array(f[\"whitened_data\"])\n",
    "        y = np.array(f[\"parameters\"])\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| var | |\n",
    "|----|----|\n",
    "|x | data|\n",
    "|y | parameters|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = load_data(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(x), np.shape(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the parameters we can see that the data is not mixed but all the signals are first, then noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is good to mix the data up first, otherwise you can run into problems when training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randidx = np.arange(len(x))\n",
    "np.random.shuffle(randidx)\n",
    "\n",
    "x = x[randidx]\n",
    "y = y[randidx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is loaded plot one of the time series.\n",
    "\n",
    "As mentioned before the sampling rate `fs` is 5mHz and the duration is 204800 seconds (~2days)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 0.005                          # sampling frequency\n",
    "duration = 204800                   # duration in seconds\n",
    "t = np.arange(0, duration, 1. / fs) # array of time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2,figsize=(8,6))\n",
    "ax[0].plot(t, x[np.where(y[:,0]==1)[0][9], 0])\n",
    "ax[1].plot(t, x[np.where(y[:,0]==0)[0][0], 0])\n",
    "for i in range(2):\n",
    "    ax[i].set_xlabel(\"Time (s)\")\n",
    "    ax[i].set_ylabel(\"Whitened strain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that the data is scaled to have values around 1 (in this case it is just Gaussian noise with a standard deviation of 1).\n",
    "\n",
    "This is an important step when training any neural network - the data preparation.\n",
    "\n",
    "The outputs after TDI are very small (~$10^{-22}$), therefore to rescale them, we whiten the data using a projected LISA power spectral density (PSD). In this case it is the same PSD that we used to generate the data, however in practice this is usually measured from the data. \n",
    "For this method of machine learning it is not as important that the measured PSD used for whitening is an accurate representation of the real PSD, as long as we perform the same data preparation beforehand and use realistic data the network should learn the underlying distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels\n",
    "\n",
    "In their original forms the labels are simply a list of zeros and ones with zero corresponding to no signal and one correponding to signal. We want the trained neural network to predict these labels as accurately as possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset we also include some of the parameters associated with the MBHB, The NaN values here are the empty signal parameter slots for the noise only data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Label vector: {y[:3]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/testing split\n",
    "\n",
    "The data is generally split into three components training, validation and test data. The training data is used to train the network and update the weights, the validation data is used to test the outputs of the network during training (the weights are not updated with this data). Finally the test data is used at the end of training to test the performance of the network.\n",
    "\n",
    "In this case we reserve 10% of the data for validation and 10% for test. This can be adjusted as you wish, in general the more training data the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = int(0.8 * x.shape[0])\n",
    "n_val = int(0.1 * x.shape[0])\n",
    "x_train, x_val, x_test = x[:n_train], x[n_train:n_train+n_val], x[n_train:n_train+2*n_val]\n",
    "y_train, y_val, y_test = y[:n_train, 0], y[n_train:n_train+n_val, 0], y[n_train:n_train+2*n_val, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## General parameters\n",
    "\n",
    "We need to define some general parameters for networks we're going to use:\n",
    "\n",
    "* Batch size: refers to the number of images/samples passed to the network in a single instance of training\n",
    "* Input shape: defines the shape of the input to the network, the batch size is ignored so we only specify the number of channels(detectors) and the length of the time series.\n",
    "* Epochs: refers to the number of times the network will train on the complete dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128            # number of samples in each batch of training data\n",
    "input_shape = [3, 1024]     # shape of the data\n",
    "n_epochs = 20               # number of epochs to train for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "\n",
    "We now need to construct the neural network that we're going to training to classify the data.\n",
    "\n",
    "We start by defining the type of pytorch model. In this case we're using `Sequential`. When using this type of model any layers we add are assumed to directly follow from the previous one. \n",
    "\n",
    "In this case we're going to use a 1D covolutional layer (`Conv1d`) layer. We need to specify the number of neurons, the size of the filter (or kernel), the activation function and the shape of the input. When running this it is a good idea to check if the channels come first or last, in this case we have channels first. i.e. `[batch_size, n_channels, n_samples]` rather than `[batch_size, n_samples, n_channels]`.\n",
    "\n",
    "We then add 1D Max-pooling which will reduce the dimensions of data being passed to the next layer by taking the maximum value in blocks of a given size (in this case 2).\n",
    "\n",
    "We the repeat the convolution + max-pooling combination again.\n",
    "\n",
    "This is followed by a `Linear` layer (or fully-connected or Dense (Keras)), which requires a flat input so we also add a flattening layer. For dense layer we only need to specifiy the number of neurons and the activation function. This flattens the inputs from `[batch_size, n_channels, n_samples]` to `[batch_size, n_samples]`.\n",
    "\n",
    "Finally the output layer. Since this is a classifcation task we want the network to predict one of the two classes, so we use two neurons. As for the activation function, we use a softmax function since this ensures the sum of all the output is one, so they can be thought of as a sort of probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can print what GPUs are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then select the GPU that we want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "          torch.nn.Conv1d(3,16,8,padding=\"same\"), # [input channels, number of filters, filter size]\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.MaxPool1d(2),                  \n",
    "          torch.nn.Conv1d(16,64,8,padding=\"same\"),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.MaxPool1d(2),\n",
    "          torch.nn.Conv1d(64,8,8,padding=\"same\"),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.MaxPool1d(2),\n",
    "          torch.nn.Flatten(),\n",
    "          torch.nn.LazyLinear(512), # LazyLinear automatically computes the input size from flattened convolutions\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.LazyLinear(128),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.LazyLinear(2)\n",
    "        ).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling\n",
    "\n",
    "It's at this stage that we specify the loss function to use and what optimiser to use.\n",
    "\n",
    "The **optimiser** is the algorithm that is used to explore the parameter space of the network weights. In this example we're going to use `Adam` with a learning rate of 0.001 and the other parameters left as their default values. There a various different optimisers to choose from but Adam has proven to be realiable for a wide variety of problems and is a good place to start.\n",
    "\n",
    "We then need to define the function that will quantify the network's performance, the **loss function**. In this case we're using **Categorical Crossentropy**. This combined with the **Softmax layer** means the network will ouput a vector of probabilities for each samples where each probability corresponds to a particular class.\n",
    "\n",
    "For this binary (noise/signal) case it can written as:\n",
    "\n",
    "$$f(\\theta) = - \\sum_{i \\in S} log(\\theta_{i}^{S}) - \\sum_{i \\in N} log(\\theta_{i}^{N})$$\n",
    "\n",
    "where $\\theta_{i}^{S/N}$ is the predicted probability of class signal-noise (S) or noise-only (N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = torch.utils.data.TensorDataset(torch.Tensor(x_train), torch.Tensor(y_train))    # convert training data to tensor dataset\n",
    "train_loader = torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, shuffle=True)  # create a `dataloader`\n",
    "\n",
    "val_dataset = torch.utils.data.TensorDataset(torch.Tensor(x_val), torch.Tensor(y_val))\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "We're now ready to train the network. We can write a small function which performs the training on each batch of data.\n",
    "\n",
    "A portion of the data (the validation set) is used to keep track of the networks performance during training. Importantly it is never used to update the networks parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(x_batch, y_batch, train=True):\n",
    "    \"\"\" Train one batch of data\"\"\"\n",
    "    model.train(train)\n",
    "    if train:\n",
    "        optimiser.zero_grad()\n",
    "    # send data to the device\n",
    "    x_batch = x_batch.to(device)\n",
    "    y_batch = y_batch.to(device)\n",
    "    # compute outputs of the model\n",
    "    outputs = model(x_batch)\n",
    "    # compute the loss after putting them through a softmax activation to normalise them between 0 and 1\n",
    "    loss = loss_func(outputs.softmax(dim=1), y_batch.to(torch.long))\n",
    "\n",
    "    outputs = (outputs[:, 1] > 0.5).float()\n",
    "    correct = (outputs == y_batch).float().sum().cpu().numpy()\n",
    "    accuracy = 100 * correct / len(y_batch)\n",
    "    \n",
    "    # if training compute the gradients and update the optimizer based on those gradients\n",
    "    if train:\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the training loss and validation loss as a function of time to see if the networks has trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracy = []\n",
    "val_accuracy = []\n",
    "# loop over the training function above for each epoch and batch\n",
    "for epoch in range(n_epochs):\n",
    "    for x_batch,y_batch in train_loader: # Loop over the batches\n",
    "        train_loss,train_acc = train_batch(x_batch.to(device), y_batch.to(device), train=True) # put batches on device and run train function\n",
    "    train_losses.append(train_loss.detach().cpu().numpy())\n",
    "    train_accuracy.append(train_acc)\n",
    "\n",
    "    # also compute loss/accuracy for the validation without computing gradients\n",
    "    with torch.no_grad():\n",
    "        for x_batch,y_batch in train_loader:\n",
    "            val_loss,val_acc = train_batch(x_batch.to(device), y_batch.to(device), train=False) # run same training loop but do not update weights\n",
    "    val_losses.append(val_loss.detach().cpu().numpy())\n",
    "    val_accuracy.append(val_acc)\n",
    "\n",
    "    print(f\"Epoch: {epoch}, Train Loss: {train_loss}, Validation Loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Now that the network in trained we can analyse the output to understand how well it has learn to predict the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n",
    "We can plot the values of the loss function we calculated to see how network learned over the epochs, we can also include the validation set which was never used to train the network.\n",
    "\n",
    "The validation curve can inform us if the network has over trained on the training data and not correctly learnt the underlying signal model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the figure\n",
    "fig, axs = plt.subplots(2, 1, figsize=(8, 10), sharex=True)\n",
    "axs = axs.ravel()     # flattens the array of axes so it's easier to index\n",
    "# plot the loss\n",
    "axs[0].plot(np.arange(n_epochs), train_losses, label='Loss')\n",
    "axs[0].plot(np.arange(n_epochs), val_losses, label='Val loss')\n",
    "# plot accuracy\n",
    "axs[1].plot(np.arange(n_epochs), train_accuracy, label='Accuracy')\n",
    "axs[1].plot(np.arange(n_epochs), val_accuracy, label='Val accuracy')\n",
    "# labels\n",
    "axs[1].set_xlabel(\"Epochs\")\n",
    "axs[0].set_ylabel(\"Loss\")\n",
    "axs[1].set_ylabel(\"Accuracy\")\n",
    "axs[0].legend()\n",
    "axs[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "\n",
    "We can easily get predictions for the testing set or any other data we might have using the trained model. We will use these to quantify the networks performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its a good idea to plot some of your test data to confirm that it appears similar to the training set, this can often be an issue when many transformations are made to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2,figsize=(8,6))\n",
    "ax[0].plot(t, x_test[np.where(y_test==1)[0][0], 0], label=\"Signal\")\n",
    "ax[1].plot(t, x_test[np.where(y_test==0)[0][0], 0], label=\"Noise\")\n",
    "for i in range(2):\n",
    "    ax[i].set_xlabel(\"Time (s)\")\n",
    "    ax[i].set_ylabel(\"Whitened strain\")\n",
    "    ax[i].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make you predicitions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x_test_tensor = torch.FloatTensor(x_test).to(device)\n",
    "    y_pred = model(x_test_tensor).cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the indices where the signals and noise are and plot the network outputs after passing through a softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noiseinds = np.where(y_test == 0)[0]\n",
    "signalinds = np.where(y_test == 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "bins = np.linspace(0,1,30)\n",
    "ax.hist(y_pred[noiseinds].softmax(-1)[:,1], alpha=0.5, bins=bins, label=\"Signal\")\n",
    "ax.hist(y_pred[signalinds].softmax(-1)[:,1], alpha=0.5, bins=bins, label=\"Noise\")\n",
    "ax.set_xlabel(\"Softmax network output\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the outputs before passing through a softmax, which can sometimes work as a better statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "bins = np.linspace(-10,40,40)\n",
    "ax.hist(y_pred[noiseinds,1], alpha=0.5, bins=bins, label=\"Signal\")\n",
    "ax.hist(y_pred[signalinds,1], alpha=0.5, bins=bins, label=\"Noise\")\n",
    "ax.set_xlabel(\"Network output\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "\n",
    "A confusion matrix allows us to visualise what classes the network is predicting correctly or incorrectly.\n",
    "\n",
    "In this case we may want to minimise the number of false positives (noise classified as signal) rather than maximise the number of true positives.\n",
    "\n",
    "Scikit-learn includes lots of useful functions to compute metrics like confusion matrices and ROC curves (which we will see later on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compute the confusion matrix we need the predicted classes rather than their probabilites. We get these by simply taking the class with the maximum probability for each time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_class = np.argmax(y_pred, axis=1)\n",
    "true_class = np.argmax(np.array([[0,1] if yt==1 else [1,0] for yt in y_test]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(true_class, pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm/sum(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC curves\n",
    "\n",
    "Reciever operator characteristic curves are another way of quatifying the network's performance. It's a plot of the true positive rate (probability of detection) against the false positive rate (probabilty of false alarm).\n",
    "\n",
    "We use Scikit learn again to compute the ROC curve, but this time we use the probabilites instead of the classes. Importantly we use the probability of the a time series being a signal (the second column in the matrix representation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [:, 1] for second column\n",
    "fa, ta, _ = metrics.roc_curve(y_test, y_pred[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.plot(fa, ta)\n",
    "plt.xlabel('False alarm probability')\n",
    "plt.ylabel('True alarm probability')\n",
    "plt.title('ROC curve ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can empirically estimate a detection thresold by setting a false alarm rate (say 1%) and measuring the statistic based on many example of the noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we get the network outputs for all the noise only inputs, arange them in order of value and take the value where 1% of the outputs is larger (i.e. 1% falsa alarm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc1_fa = sorted(y_pred[noiseinds,1])[int(0.99*len(y_pred[noiseinds,1]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_pred[:,1]<perc1_fa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "bins = np.linspace(y_pred.min(), y_pred.max(), 40)\n",
    "ax.hist(y_pred[noiseinds,1], alpha=0.5, label=\"noise\", bins=bins)\n",
    "ax.hist(y_pred[signalinds,1], alpha=0.5, label=\"signal\", bins=bins)\n",
    "ax.axvline(perc1_fa, color=\"red\", label=\"1% false alarm\")\n",
    "ax.set_xlabel(\"ML statistic\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try and write some code to produce point estimates of some of the parameters of the waveforms as well as a detection statistic.\n",
    "i.e. try and predict the mass as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.6.11 ('a2_labs')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "8f6092ff7a8cdac9d8834511b09574d1caac12cb85df297a9760cf0bc804fce8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
